from LLM_patch_generation.args_parser import parse_arguments_validator
from LLM_patch_generation.generator_utils import ask_LLM
from LLM_patch_generation.extract_info.env_scanner import extract_environment_info
from LLM_patch_generation.extract_info.container_scanner import extract_container_info, list_containers
from LLM_patch_generation.validator_utils import search_for, generate_validator_prompt
from os import mkdir
import time

# LLM model tasked to validate generated correction patches
VALIDATOR_MODEL = "gpt-5.1"
VALIDATOR_OUTPUT_DIR = "validator_output"

def main():
    # Setting directory containing correction patches
    args = parse_arguments_validator()
    patches_filepath = args.Patches_directory_filepath


    # Specifying targeted vulnerability 
    print('Identifying vulnerability...')
    try:
        with open(f'{patches_filepath}/deepseek-R1_details.txt', 'r') as f:
            for line in f:
                if line.startswith('Vulnerability:'):
                    TARGET_VULNEARBILITY = line.split(':', 1)[1].strip()
                    break
    except FileNotFoundError:
        print("Could not find directory containing correction patches. Ending program.")
        return


    # Setting ENVIRONMENT INFORMATION for environment where vulnerability is located
    valid_user_input = False
    while not valid_user_input:
        user_input = input('Is vulnerability found in a Docker Container [Y/n]? ')
        match user_input.lower():
            case 'y':
                vuln_in_container = True
                valid_user_input = True
            case 'n':
                vuln_in_container = False
                valid_user_input = True
            case _:
                pass


    print('Extracting environment information...')
    
    if vuln_in_container:
        # Deactivated for testing:

        #if vuln_in_container:
            #active_containers = list_containers()
            
            #print('Select container from list: ')
            #for container in active_containers:
                #print(f'- {container}')
            
            #valid_user_input = False
            #while not valid_user_input:
                #user_input = input()

                #if user_input in active_containers:
                    #env_info = extract_container_info(user_input)
                    #valid_user_input = True
                #else:
                    #print('Invalid input. Select container from list')
        #else:
            #ENVIRONMENT_INFORMATION = extract_environment_info()

        environment_pattern = r"^\s*\"" + TARGET_VULNEARBILITY + r"\":\s*\{(.*?)\}"
        match = search_for(environment_pattern, 'env_info.txt')
        
        if match:
            ENVIRONMENT_INFORMATION = match
        else:
            print("Target vulnerability not found in env_info.txt. Ending program.")
            return
    else:
        environment_pattern = r"^\s*\"" + "pop-os-24-ambient" + r"\":\s*\{(.*?)\}"
        match = search_for(environment_pattern, 'env_info.txt')
        
        if match:
            ENVIRONMENT_INFORMATION = match
        else:
            print("Target vulnerability not found in env_info.txt. Ending program.")
            return


    # Gathering cheatsheet content, including solution
    '''
    print('Locating vulnerability in cheatsheet...')

    
    vuln_cheats_pattern = r"^\s*\"" + TARGET_VULNEARBILITY + r"\":\s*\{(.*?)\}"
    match = search_for(vuln_cheats_pattern, 'cheatsheet.txt')

    if match:
        VULNERABILITY_CHEATS = match
    else:
        print("Vulnerability not found in cheatsheet. Ending program.")
        return
    '''
    
    # Storing all generated correction patches in a single variable
    print('Gathering generated correction patches...')
    MODELS = ['gemini-2.5-pro', 'gemini-2.5-flash', 'deepseek-R1', 'deepseek-V3.1']
    patches = {}

    for model in MODELS:
        try:
            with open(f'{patches_filepath}/{model}_patch.clean.sh', 'r') as f:
                content = f.read()
                patches[model] = content
        except FileNotFoundError:
            print(f"Could not find correction patch generated by {model}. Ending program.")
            return
    
    GENERATED_CORRECTION_PATCHES = ''
    GENERATED_CORRECTION_PATCHES +=  f'===============================================================================\n'
    for model, patch in patches.items():
        GENERATED_CORRECTION_PATCHES +=  f'CORRECTION PATCH GENERATED BY {model}:\n\n'
        GENERATED_CORRECTION_PATCHES +=  f'{patch}\n'
        GENERATED_CORRECTION_PATCHES +=  f'===============================================================================\n'
    
    # Fully assembling prompt to feed the validator
    validator_prompt = generate_validator_prompt(ENVIRONMENT_INFORMATION, VULNERABILITY_CHEATS, GENERATED_CORRECTION_PATCHES)

    timer_start = time.perf_counter()

    # Sending prompt to validator API
    print(f"Requesting verdict from {VALIDATOR_MODEL}...")
    response = ask_LLM(VALIDATOR_MODEL, validator_prompt)

    if response.status == "ERR":
        print(f"ERROR while fetching response. Shutting down script.")
        print(f"ERROR details: {response.content}")
        return

    timer_end = time.perf_counter()
    elapsed_time = (timer_end - timer_start)
    
    # Saving validator verdict
    print("Saving validator output...")
    try:
        mkdir(VALIDATOR_OUTPUT_DIR)
    except FileExistsError:
        pass

    filename = f"{TARGET_VULNEARBILITY}_verdict.txt"

    with open(f"{VALIDATOR_OUTPUT_DIR}/{filename}", "w") as f:
        f.write(response.content)
        f.write(f"\n\nELAPSED TIME: {elapsed_time}s")
        print(f"Verdict successfully saved at {VALIDATOR_OUTPUT_DIR}/{filename}!")

    
if __name__ == '__main__':
    main()